import numpy as np
import faiss
from qdrant_client import QdrantClient
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama
import gradio as gr

class SearchAndDisplay:
    def __init__(self, faiss_index_path, qdrant_url, embedding_model, llm_model):
        # Initialize embedding model
        self.embeddings = OllamaEmbeddings(model=embedding_model)
        # Initialize FAISS index
        self.index = faiss.read_index(faiss_index_path)
        # Initialize Qdrant client
        self.client = QdrantClient(url=qdrant_url)
        # Initialize LLaMA model
        self.llm = Ollama(model=llm_model)

    def embed_query(self, query):
        """Generate embedding for a query."""
        return self.embeddings.embed_query(query)

    def search_and_rerank(self, query, k=50, top_n=3):
        """Search the Faiss index and rerank results using Qdrant."""
        query_embedding = self.embed_query(query)
        query_embedding = np.array([query_embedding])

        # Search Faiss index
        distances, indices = self.index.search(query_embedding, k)
        result_ids = indices[0].tolist()
        results = self.client.retrieve(collection_name="documents_collection", ids=result_ids)

        # Take the top N results
        top_results = results[:top_n]

        # Generate embeddings for reranking
        mutable_results = []
        for result in top_results:
            embedding = self.embed_query(result.payload['full_text'])
            mutable_result = {"record": result, "embedding": embedding}
            mutable_results.append(mutable_result)

        # Rerank the results
        query_embedding = np.squeeze(query_embedding)
        reranked_results = self.rerank(mutable_results, query_embedding)

        return reranked_results

    @staticmethod
    def cosine_similarity(vector_a, vector_b):
        """Compute the cosine similarity between two vectors."""
        vector_b = np.squeeze(vector_b)
        dot_product = np.dot(vector_a, vector_b)
        norm_a = np.linalg.norm(vector_a)
        norm_b = np.linalg.norm(vector_b)
        return dot_product / (norm_a * norm_b)

    def rerank(self, results, query_embedding):
        """Rerank results based on cosine similarity."""
        return sorted(
            results,
            key=lambda result: self.cosine_similarity(result['embedding'], query_embedding),
            reverse=True
        )

    def search_and_display_results(self, query):
        """Perform search and display formatted results using LLaMA."""
        try:
            # Perform search and rerank
            reranked_results = self.search_and_rerank(query)

            # Format the results
            saved_results = []
            for result in reranked_results:
                record = result['record']
                formatted_result = (
                    f"Author: [{record.payload['author']}]({record.payload['url']})\n"
                    f"Title: {record.payload['title']}\n"
                    f"Date: {record.payload['publish_date']}\n"
                    f"Text: {record.payload['full_text'][:200]}...\n"  # Truncate text for brevity
                )
                saved_results.append(formatted_result)

            # Combine the formatted results
            saved_results_combined = "\n\n".join(saved_results)

            output_prompt = f"""
            Please present the following top 3 search results in a clean and readable format. 
            Each result should be clearly separated with headers and bullet points, 
            and should be formatted as follows:

            Author: [AUTHOR_NAME](URL)
            Title: TITLE_HERE
            Date: DATE_HERE
            Text: Full text.

            {saved_results_combined}

            Make sure that the author's name is hyperlinked to the corresponding URL, and that the date is formatted properly.
            Each Author's name should be bolded, and the entire response should be returned in Markdown format.

            Make it in a modern way.

            Lastly, write: "I hope this is what you were looking for!"
            """

            # Generate the response
            response_llama = self.llm.generate(
                prompts=[output_prompt],
                tags=["example_run"],
                metadata={"source": "user_query"}
            )

            # Return the response generated by LLaMA3
            return response_llama.generations[0][0].text

        except Exception as e:
            return f"An error occurred: {str(e)}"

# Instantiate the combined class
search_and_display = SearchAndDisplay(
    faiss_index_path="../indexing/faiss_index.index",
    qdrant_url="http://localhost:6333",
    embedding_model="nomic-embed-text",
    llm_model="llama3"
)

# Gradio interface
iface = gr.Interface(
    fn=search_and_display.search_and_display_results,
    inputs="text",
    outputs="markdown",
    title="Hugging Face Search Engine",
    description="Enter a search query and retrieve the top 3 results, formatted by LLaMA3."
)

if __name__ == "__main__":
    iface.launch(share=True)
